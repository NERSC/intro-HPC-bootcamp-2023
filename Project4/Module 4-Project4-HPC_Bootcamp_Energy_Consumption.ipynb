{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Module 4: Project 4-Introduction to High-Performance Computing (HPC) and Energy Consumption in Programming\nHigh-Performance Computing (HPC) refers to the utilization of parallel processing techniques to execute computational tasks efficiently and quickly. It involves the use of supercomputers and parallel processing techniques for solving complex computational problems.  In this notebook, you will be introduced and exploring concepts related to parallel programming paradigms in HPC.\n![HPC Analysis](https://www.nersc.gov/assets/Uploads/Perlmutter-panel-art-5-2021__ResizedImageWzYwMCwyMjVd.jpg)\n## Energy Consumption in HPC\nEnergy consumption is a critical aspect of HPC. With the growing demand for computational power, the energy required to fuel HPC systems has also increased. Understanding and optimizing energy consumption is vital for both economic and environmental reasons.\n\nIn this notebook, we will explore the concepts related to HPC with a focus on energy consumption. We will provide examples that demonstrate the use of MPI (Message Passing Interface) and CUDA (Compute Unified Device Architecture) in HPC systems.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "dec6ddec-3952-4547-867c-c6cbe8f97b00"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Parallel Programming\n",
        "Parallel programming is a programming model wherein the execution flow of the application is divided into multiple concurrent threads to increase the computational speed and solve larger problems. It is widely used in high-performance computing (HPC) to perform complex computations more efficiently.\n",
        "\n",
        "In parallel programming, tasks are divided into subtasks that are processed simultaneously (in parallel) on different processors or cores in a computer. This approach can significantly reduce the execution time for large-scale computational problems.\n",
        "\n",
        "![Parallel Programming](https://miro.medium.com/max/700/1*QV1NtKlFP2fngZ3mXCtmOQ.png)\n",
        "\n",
        "In the image above, the same task is divided into four subtasks that are executed simultaneously, resulting in a significant reduction in total execution time.\n",
        "\n",
        "Parallel programming is essential in HPC because it allows us to leverage the full potential of modern multi-core and multi-processor systems. It's used in a variety of applications, including simulations, data analysis, machine learning, and many others."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "c48e06da-6972-46a5-8765-47d2086e163d"
    },
    {
      "cell_type": "markdown",
      "source": "# Introduction to MPI (Message Passing Interface)\nMPI is a standardized and portable message-passing system designed to function on a wide variety of parallel computing architectures. The standard defines the syntax and semantics of a core of library routines useful to a wide range of users writing portable message-passing programs in C, C++, and Fortran. There are several well-tested and efficient implementations of MPI, including some that are free or in the public domain.\n\nMPI programs are parallel applications that use a SPMD (single program, multiple data) model. This means that the same program is executed on all processors, and tasks are divided among the processors to be executed simultaneously.\n\n![MPI](https://www.hpcwire.com/wp-content/uploads/2017/05/MPIlogo2.gif)\n\nIn the image above, each processor is running the same program but is working on a different part of the data. The processors communicate with each other by sending and receiving messages, which are coordinated by the MPI library.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "a0e91b5a-90b0-4136-a70e-7c778d2b81ee"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1: MPI (Message Passing Interface) in Python\n",
        "MPI is a standardized and portable message-passing system designed to function on parallel computers. It enables processes to communicate by sending and receiving messages.\n",
        "\n",
        "### Calculating π (pi) using MPI and the Monte Carlo Method\n",
        "We'll demonstrate a simple MPI program that calculates the value of π (pi) using the Monte Carlo method. This method utilizes random sampling to obtain numerical results for mathematical problems.\n",
        "\n",
        "Note: To run the following code, you'll need to have an MPI environment set up, and the code must be executed using the `mpiexec` command."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "a40f0abd-322d-440d-86ec-e063ef29a5b6"
    },
    {
      "cell_type": "code",
      "source": [
        "from mpi4py import MPI\n",
        "import random\n",
        "\n",
        "def calculate_pi(rank, size):\n",
        "    random.seed(rank)\n",
        "    inside_circle = 0\n",
        "    total_points = 1000000\n",
        "    points_per_process = total_points // size\n",
        "\n",
        "    for _ in range(points_per_process):\n",
        "        x, y = random.random(), random.random()\n",
        "        if x**2 + y**2 <= 1:\n",
        "            inside_circle += 1\n",
        "\n",
        "    return 4 * inside_circle / points_per_process\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "local_pi = calculate_pi(rank, size)\n",
        "global_pi = comm.reduce(local_pi, op=MPI.SUM, root=0)\n",
        "\n",
        "if rank == 0:\n",
        "    print('Estimated value of π:', global_pi / size)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "80d9cccd-dda6-4b69-87a1-07c8f04e461e"
        },
        "ExecuteTime": {
          "end_time": "2023-08-06T11:53:16.386951+00:00",
          "start_time": "2023-08-06T11:53:15.813324+00:00"
        }
      },
      "id": "082edfd2-fec2-4dee-9e4b-c7bec9889144"
    },
    {
      "cell_type": "markdown",
      "source": "# Introduction to CUDA (Compute Unified Device Architecture)\nCUDA is a parallel computing platform and programming model developed by NVIDIA for general computing on its own GPUs (graphics processing units). CUDA enables developers to speed up compute-intensive applications by harnessing the power of GPUs for the parallelizable part of the computation.\n\n![CUDA](https://assets.nvidiagrid.net/ngc/logos/Cuda.png)\n\n\nIn a CUDA program, the CPU (referred to as the host) and the GPU (referred to as the device) work together. The sequential part of the application still runs on the CPU, and the computationally-intensive part is offloaded to the GPU.\n\n",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "6239b5f5-b240-4c7e-845c-df7333ce48e1"
    },
    {
      "cell_type": "markdown",
      "source": "## Example 2: Using CUDA in Python\nCUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA. It allows developers to use NVIDIA GPUs for general-purpose computing.\n\n### Matrix Multiplication using CUDA\nIn this example, we'll demonstrate how to use CUDA in Python to perform matrix multiplication. This operation is a common task in scientific computing and can be parallelized efficiently on GPUs.\n\nNote: To run the following code, you'll need to have a compatible NVIDIA GPU and the required CUDA libraries installed.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "ad6b1d10-d0c0-411e-b358-54a27369380c"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def matrix_mul_cuda(A, B, C):\n",
        "    row, col = cuda.grid(2)\n",
        "    if row < C.shape[0] and col < C.shape[1]:\n",
        "        tmp = 0.\n",
        "        for k in range(A.shape[1]):\n",
        "            tmp += A[row, k] * B[k, col]\n",
        "        C[row, col] = tmp\n",
        "\n",
        "A = np.random.rand(24, 12)\n",
        "B = np.random.rand(12, 22)\n",
        "C = np.zeros((24, 22))\n",
        "\n",
        "d_A = cuda.to_device(A)\n",
        "d_B = cuda.to_device(B)\n",
        "d_C = cuda.device_array(C.shape, np.float64)\n",
        "\n",
        "threadsperblock = (16, 16)\n",
        "blockspergrid_x = int(np.ceil(A.shape[0] / threadsperblock[0]))\n",
        "blockspergrid_y = int(np.ceil(B.shape[1] / threadsperblock[1]))\n",
        "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
        "\n",
        "matrix_mul_cuda[blockspergrid, threadsperblock](d_A, d_B, d_C)\n",
        "C = d_C.copy_to_host()\n",
        "\n",
        "print('C =', C)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "4546cb4e-87d4-4429-b8e5-3c4224d7b6ba"
        },
        "ExecuteTime": {
          "end_time": "2023-08-06T11:54:34.053328+00:00",
          "start_time": "2023-08-06T11:54:33.109132+00:00"
        }
      },
      "id": "64f98767-917a-43d3-a689-b5cf411fb2d2"
    },
    {
      "cell_type": "markdown",
      "source": "## Example 3: Combining MPI and CUDA for Parallel Computing\nIn this example, we'll demonstrate how to combine MPI and CUDA to perform parallel computing across multiple GPUs. We'll use MPI to distribute the work among different processes, and within each process, we'll use CUDA to perform computations on the GPU.\n\n### Vector Addition using MPI and CUDA\nWe'll create a simple program that performs vector addition across multiple GPUs using both MPI and CUDA.\n\nNote: To run the following code, you'll need to have a multi-GPU setup and the required MPI and CUDA libraries installed. Also need to choose the exclusive GPU configuration settings when starting JupyterHub on Perlmutter.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "3890e11c-de5d-4ba8-9b4a-c0e40162f32c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining MPI and CUDA for Parallel Computing\n",
        "In some cases, we can combine MPI and CUDA to perform parallel computing across multiple GPUs. We use MPI to distribute the work among different processes, and within each process, we use CUDA to perform computations on the GPU.\n",
        "\n",
        "This approach allows us to leverage the full potential of multi-GPU setups, where each GPU can be working on a different part of the data simultaneously. This can lead to significant reductions in execution time for large-scale computational problems.\n",
        "\n",
        "![MPI and CUDA](https://www.olcf.ornl.gov/wp-content/uploads/2011/08/CUDA_Aware_MPI_1.png)\n",
        "\n",
        "In the image above, each MPI process is associated with a different GPU. Each process performs computations on its GPU, and the processes communicate with each other via MPI."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "88754caa-b92c-447b-b4ec-eb586a1dfa47"
    },
    {
      "cell_type": "code",
      "source": [
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def vector_add_cuda(A, B, C):\n",
        "    i = cuda.grid(1)\n",
        "    if i < C.size:\n",
        "        C[i] = A[i] + B[i]\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "N = 1000000\n",
        "A = np.random.rand(N).astype(np.float32)\n",
        "B = np.random.rand(N).astype(np.float32)\n",
        "C = np.zeros_like(A)\n",
        "\n",
        "start = rank * N // size\n",
        "end = (rank + 1) * N // size\n",
        "\n",
        "d_A = cuda.to_device(A[start:end])\n",
        "d_B = cuda.to_device(B[start:end])\n",
        "d_C = cuda.device_array_like(d_A)\n",
        "\n",
        "threadsperblock = 1024\n",
        "blockspergrid = (d_A.size + (threadsperblock - 1)) // threadsperblock\n",
        "\n",
        "vector_add_cuda[blockspergrid, threadsperblock](d_A, d_B, d_C)\n",
        "C[start:end] = d_C.copy_to_host()\n",
        "\n",
        "comm.Allreduce(MPI.IN_PLACE, C, op=MPI.SUM)\n",
        "\n",
        "if rank == 0:\n",
        "    print('C =', C)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "68943e2b-0b7c-4d29-b56a-770a582f5bb7"
    }
  ],
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "d06b173b-5588-5003-a8f5-626eb754c05f",
        "openai_ephemeral_user_id": "27b37775-cc2a-5458-bffc-e8b390507295",
        "openai_subdivision1_iso_code": "US-CA"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "noteable": {
      "last_transaction_id": "27070cc5-c70a-42e7-998d-bf0e4f4d1d80",
      "last_delta_id": "d168be69-cf4d-4177-aed1-8039a3188212"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small",
    "nteract": {
      "version": "noteable@2.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}