{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module 4: Project 5- Introduction to Parallel Programming and Performance Optimization on HPC Systems with respect to Energy Consumption\n",
        "\n",
        "In this module, participants will learn the principles of parallel programming and performance optimization techniques for HPC systems. They will explore how to leverage parallelism to improve code execution on distributed memory systems using MPI and on GPUs using CUDA. The module will include three examples, each with an original application code and an optimized version demonstrating improvements in execution time and energy consumption.\n",
        "\n",
        "## Outline\n",
        "\n",
        "1. Example 1: Python MPI Optimization\n",
        "2. Example 2: Python CUDA Optimization\n",
        "3. Example 3: Python MPI + CUDA Optimization"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "5fac4c42-4c83-4d50-b1be-bba7f8400676"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1: Python MPI Optimization\n",
        "\n",
        "### Introduction to MPI and parallel programming concepts\n",
        "\n",
        "Message Passing Interface (MPI) is a standardized and portable message-passing system designed to function on a wide variety of parallel computing architectures. The standard defines the syntax and semantics of a core of library routines useful to a wide range of users writing portable message-passing programs in C, C++, and Fortran. There are several well-tested and efficient implementations of MPI, many of which are open-source or in the public domain. These fostered the development of a parallel software industry, and encouraged development of portable and scalable large-scale parallel applications.\n",
        "\n",
        "![MPI](https://upload.wikimedia.org/wikipedia/commons/thumb/e/eb/MPI_logo.svg/1200px-MPI_logo.svg.png)\n",
        "\n",
        "### Analyzing the original Python MPI code\n",
        "\n",
        "We will start by analyzing a simple MPI code written in Python. This code will demonstrate the basic MPI operations such as sending and receiving messages between different processes.\n",
        "\n",
        "### Identifying bottlenecks and performance issues\n",
        "\n",
        "After understanding the basic operations, we will use profiling tools to identify the bottlenecks and performance issues in the code. This will give us insights on which parts of the code we need to optimize.\n",
        "\n",
        "### Implementing MPI communication improvements\n",
        "\n",
        "Based on the identified bottlenecks, we will implement improvements in the MPI communication. This can include techniques such as non-blocking communication, collective communication, and optimizing the communication topology.\n",
        "\n",
        "### Measuring and comparing the performance of the optimized code\n",
        "\n",
        "Finally, we will measure the performance of the optimized code and compare it with the original code. This will demonstrate the effectiveness of the optimization techniques."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "184fdaed-4ce8-40ad-ad12-aa6d0e624a63"
    },
    {
      "cell_type": "markdown",
      "source": "## Example 2: Python CUDA Optimization\n\n### Introduction to CUDA programming and GPU architecture\n\nCUDA is a parallel computing platform and application programming interface (API) model created by Nvidia. It allows software developers to use a CUDA-enabled graphics processing unit (GPU) for general purpose processing â€“ an approach known as GPGPU (General-Purpose computing on Graphics Processing Units). The CUDA platform is designed to work with programming languages such as C, C++, and Fortran. This accessibility makes it easier for specialists in parallel programming to use GPU resources, in contrast to prior APIs like Direct3D and OpenGL, which required advanced skills in graphics programming.\n\n![CUDA](https://developer.nvidia.com/sites/default/files/pictures/2018/parallel-computing-cuda-software-ecosystem.png)\n\n### Understanding the original Python CUDA code\n\nWe will start by understanding a simple CUDA code written in Python. This code will demonstrate the basic CUDA operations such as defining a kernel function, allocating GPU memory, and launching the kernel.\n\n### Exploring memory access patterns and cache optimization\n\nWe will then explore the memory access patterns in the code and how they affect the performance. We will also discuss how to optimize the cache usage in CUDA.\n\n### Applying loop unrolling and other memory optimization techniques\n\nBased on the memory access patterns, we will apply optimization techniques such as loop unrolling and memory coalescing. These techniques can significantly improve the performance of the code.\n\n### Benchmarking the performance and energy efficiency of the optimized code\n\nFinally, we will benchmark the performance and energy efficiency of the optimized code and compare it with the original code. This will demonstrate the effectiveness of the optimization techniques.\n\nNote: To run the following Example 2 codes, you'll need to have a compatible NVIDIA GPU and the required CUDA libraries installed.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "7579fc1f-61cd-4487-960e-5f66738cf5a5"
    },
    {
      "cell_type": "markdown",
      "source": "## Example 3: Python MPI + CUDA Optimization\n\n### Combining MPI and CUDA for distributed GPU computing\n\nIn this section, we will discuss how to combine MPI and CUDA for distributed GPU computing. This approach allows us to leverage the power of multiple GPUs in a distributed system.\n\n![MPI+CUDA](https://www.olcf.ornl.gov/wp-content/uploads/2011/08/CUDA_Aware_MPI1.png)\n\n### Reviewing the original Python MPI+CUDA code\n\nWe will start by reviewing a Python code that uses both MPI and CUDA. This code will demonstrate how to perform parallel computation on multiple GPUs using MPI for inter-GPU communication and CUDA for GPU computation.\n\n### Optimizing MPI communication and GPU memory transfers\n\nWe will then optimize the MPI communication and GPU memory transfers in the code. This can involve techniques such as overlapping communication and computation, using CUDA-aware MPI, and optimizing the communication pattern.\n\n### Utilizing asynchronous execution for better performance\n\nWe will also discuss how to utilize asynchronous execution in CUDA and MPI to achieve better performance. This can involve techniques such as using CUDA streams and non-blocking MPI communication.\n\n### Evaluating the improvements achieved in terms of performance and energy efficiency\n\nFinally, we will evaluate the improvements achieved in terms of performance and energy efficiency. We will compare the performance and energy consumption of the optimized code with the original code.\n\nNote: To run the following Example 3 codes, you'll need to have a multi-GPU setup and the required MPI and CUDA libraries installed. Also need to choose the exclusive GPU configuration settings when starting JupyterHub on Perlmutter.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "8e09c439-49dd-4d7f-8c79-aea7132a385b"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Python MPI Optimization\n",
        "# Original Code\n",
        "\n",
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "if rank == 0:\n",
        "    data = {'a': 7, 'b': 3.14}\n",
        "    comm.send(data, dest=1, tag=11)\n",
        "elif rank == 1:\n",
        "    data = comm.recv(source=0, tag=11)\n",
        "\n",
        "# This is a simple MPI code where process 0 sends a dictionary to process 1.\n",
        "# The communication is blocking, which means process 0 will wait until the message is received by process 1."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "cdf0c047-0677-48e3-879b-b025d392245d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Python MPI Optimization\n",
        "# Optimized Code\n",
        "\n",
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "if rank == 0:\n",
        "    data = {'a': 7, 'b': 3.14}\n",
        "    req = comm.isend(data, dest=1, tag=11)\n",
        "    req.wait()\n",
        "elif rank == 1:\n",
        "    req = comm.irecv(source=0, tag=11)\n",
        "    data = req.wait()\n",
        "\n",
        "# In the optimized code, we use non-blocking communication.\n",
        "# This means process 0 can continue to do other work while the message is being sent."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "52ba5d2b-1f7e-4a4d-994d-5d11900e4f38"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Python CUDA Optimization\n",
        "# Original Code\n",
        "\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def add_kernel(x, y, out):\n",
        "    tx = cuda.threadIdx.x\n",
        "    ty = cuda.blockIdx.x\n",
        "    bw = cuda.blockDim.x\n",
        "    i = tx + ty * bw\n",
        "\n",
        "    if i < x.size:\n",
        "        out[i] = x[i] + y[i]\n",
        "\n",
        "x = np.arange(100).astype(np.float32)\n",
        "y = 2 * x\n",
        "out = np.empty_like(x)\n",
        "\n",
        "threadsperblock = 32\n",
        "blockspergrid = (x.size + (threadsperblock - 1)) // threadsperblock\n",
        "add_kernel[blockspergrid, threadsperblock](x, y, out)\n",
        "\n",
        "# This is a simple CUDA code where we define a kernel function to add two arrays.\n",
        "# The kernel is launched with a certain number of thread blocks and threads per block."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "3dbcd638-9f10-4f09-883c-bc4eceac3912"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Python CUDA Optimization\n",
        "# Optimized Code\n",
        "\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def add_kernel(x, y, out):\n",
        "    tx = cuda.threadIdx.x\n",
        "    ty = cuda.blockIdx.x\n",
        "    bw = cuda.blockDim.x\n",
        "    i = tx + ty * bw\n",
        "\n",
        "    if i < x.size:\n",
        "        out[i] = x[i] + y[i]\n",
        "\n",
        "x_device = cuda.to_device(x)\n",
        "y_device = cuda.to_device(y)\n",
        "out_device = cuda.device_array_like(x)\n",
        "\n",
        "threadsperblock = 32\n",
        "blockspergrid = (x.size + (threadsperblock - 1)) // threadsperblock\n",
        "add_kernel[blockspergrid, threadsperblock](x_device, y_device, out_device)\n",
        "\n",
        "out = out_device.copy_to_host()\n",
        "\n",
        "# In the optimized code, we transfer the data to the GPU before launching the kernel and\n",
        "# transfer the result back to the host after the kernel execution.\n",
        "# This can reduce the overhead of data transfer between the host and the device."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "e38ad916-644f-4a0f-bcb1-2e6f2f517ad6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Python MPI + CUDA Optimization\n",
        "# Original Code\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def add_kernel(x, y, out):\n",
        "    tx = cuda.threadIdx.x\n",
        "    ty = cuda.blockIdx.x\n",
        "    bw = cuda.blockDim.x\n",
        "    i = tx + ty * bw\n",
        "\n",
        "    if i < x.size:\n",
        "        out[i] = x[i] + y[i]\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "x = np.arange(100).astype(np.float32)\n",
        "y = 2 * x\n",
        "out = np.empty_like(x)\n",
        "\n",
        "if rank == 0:\n",
        "    comm.send(x, dest=1, tag=11)\n",
        "    comm.send(y, dest=1, tag=12)\n",
        "elif rank == 1:\n",
        "    x = comm.recv(source=0, tag=11)\n",
        "    y = comm.recv(source=0, tag=12)\n",
        "\n",
        "    threadsperblock = 32\n",
        "    blockspergrid = (x.size + (threadsperblock - 1)) // threadsperblock\n",
        "    add_kernel[blockspergrid, threadsperblock](x, y, out)\n",
        "\n",
        "# This is a simple MPI+CUDA code where process 0 sends two arrays to process 1,\n",
        "# and process 1 uses CUDA to add the two arrays."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "227614b0-758e-4568-8d21-3e892502c492"
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Python MPI + CUDA Optimization\n",
        "# Optimized Code\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def add_kernel(x, y, out):\n",
        "    tx = cuda.threadIdx.x\n",
        "    ty = cuda.blockIdx.x\n",
        "    bw = cuda.blockDim.x\n",
        "    i = tx + ty * bw\n",
        "\n",
        "    if i < x.size:\n",
        "        out[i] = x[i] + y[i]\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "x = np.arange(100).astype(np.float32)\n",
        "y = 2 * x\n",
        "out = np.empty_like(x)\n",
        "\n",
        "if rank == 0:\n",
        "    req1 = comm.isend(x, dest=1, tag=11)\n",
        "    req2 = comm.isend(y, dest=1, tag=12)\n",
        "    req1.wait()\n",
        "    req2.wait()\n",
        "elif rank == 1:\n",
        "    req1 = comm.irecv(source=0, tag=11)\n",
        "    req2 = comm.irecv(source=0, tag=12)\n",
        "    x = req1.wait()\n",
        "    y = req2.wait()\n",
        "\n",
        "    x_device = cuda.to_device(x)\n",
        "    y_device = cuda.to_device(y)\n",
        "    out_device = cuda.device_array_like(x)\n",
        "\n",
        "    threadsperblock = 32\n",
        "    blockspergrid = (x.size + (threadsperblock - 1)) // threadsperblock\n",
        "    add_kernel[blockspergrid, threadsperblock](x_device, y_device, out_device)\n",
        "\n",
        "    out = out_device.copy_to_host()\n",
        "\n",
        "# In the optimized code, we use non-blocking communication for MPI and transfer the data to the GPU before launching the kernel.\n",
        "# We also transfer the result back to the host after the kernel execution.\n",
        "# This can reduce the overhead of data transfer and improve the performance."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "4f2a60ea-8c00-42b8-ad5e-89f4cae273f3"
    }
  ],
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "3b256b22-c41b-59b8-8eb4-df760218a33b",
        "openai_ephemeral_user_id": "27b37775-cc2a-5458-bffc-e8b390507295",
        "openai_subdivision1_iso_code": "US-CA"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "noteable": {
      "last_transaction_id": "4e464a42-e67a-4242-8afb-9b34d10f82ea"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small",
    "nteract": {
      "version": "noteable@2.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}