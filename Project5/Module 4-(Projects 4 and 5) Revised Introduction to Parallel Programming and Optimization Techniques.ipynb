{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module 4 (Project 4 and 5)- Introduction to HPC in C/C++\n",
        "\n",
        "High-Performance Computing (HPC) is a discipline that focuses on the design and implementation of high-performance systems and algorithms. With the increasing demand for computational power, understanding parallel programming and optimization techniques in HPC becomes crucial.\n",
        "\n",
        "In this notebook, we will introduce parallel programming concepts, specifically focusing on OpenMP, MPI, and CUDA using the C language and cuda libraries. Additionally, we will delve into optimization techniques to enhance the performance of these parallel programs.\n",
        "\n",
        "For each parallel programming concept, we will provide:\n",
        "1. An original C example demonstrating the concept.\n",
        "2. An optimized version of the original code, showcasing how performance can be improved.\n",
        "\n",
        "Let's dive in!"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "2e699aff-946b-45fe-b215-b85c57dd1a52"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Basic Concepts of Parallel Programming\n",
        "\n",
        "Parallel programming is a technique where a problem is divided into discrete parts that can be solved concurrently. Each part is further broken down into a series of instructions. These instructions from each part can be executed simultaneously on different processors. In the context of supercomputing, parallel programming is essential to harness the full power of the hardware and achieve high computational performance.\n",
        "\n",
        "The primary goal of parallel programming is to improve the computational performance by utilizing multiple processors or cores. However, it's not just about speeding up the computation. It's also about solving larger problems, simulating more complex systems, and analyzing larger datasets.\n",
        "\n",
        "In the context of C/C++, several libraries and frameworks facilitate parallel programming. Some of the most popular ones include OpenMP for shared memory parallelism, MPI for distributed memory parallelism, and CUDA for GPU acceleration. On the Perlmutter system at NERSC, you can leverage all these paradigms to optimize your applications for high performance."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "f1773f5f-2582-4e85-87c5-14f63113f966"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to C Compiling and Executing  with OpenMP Threads (Optional)\n",
        "\n",
        "In this tutorial, you'll learn how to compile and execute a C application that utilizes OpenMP threads for parallel execution. OpenMP is a widely used library for parallel programming that allows you to easily add parallelism to your code using compiler directives.\n",
        "\n",
        "**Step 1: Write the C Application**\n",
        "\n",
        "Let's start by writing a simple C program that uses OpenMP to parallelize a loop. Create a file named `openmp_example.c` and add the following code:\n",
        "\n",
        "```c\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "\n",
        "int main() {\n",
        "    int num_threads = 4; // Number of threads\n",
        "\n",
        "    // Set the number of OpenMP threads\n",
        "    omp_set_num_threads(num_threads);\n",
        "\n",
        "    #pragma omp parallel\n",
        "    {\n",
        "        int thread_id = omp_get_thread_num();\n",
        "        printf(\"Hello from thread %d\\n\", thread_id);\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "```\n",
        "\n",
        "**Step 2: Compile the C Application**\n",
        "\n",
        "Open a terminal and navigate to the directory where your `openmp_example.c` file is located. Use the following command to compile the program with OpenMP support:\n",
        "\n",
        "```bash\n",
        "gcc -o openmp_example openmp_example.c -fopenmp\n",
        "```\n",
        "\n",
        "**Step 3: Execute the Compiled Program**\n",
        "\n",
        "After successful compilation, you'll have an executable named `openmp_example`. Run the program using the following command:\n",
        "\n",
        "```bash\n",
        "./openmp_example\n",
        "```\n",
        "\n",
        "You should see output similar to the following, indicating that different threads are printing messages concurrently:\n",
        "\n",
        "```\n",
        "Hello from thread 0\n",
        "Hello from thread 1\n",
        "Hello from thread 2\n",
        "Hello from thread 3\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "- The `#pragma omp parallel` directive creates a parallel region, and the block of code within it will be executed by multiple threads.\n",
        "- The `omp_get_thread_num()` function returns the thread ID of the current thread.\n",
        "- The `omp_set_num_threads(num_threads)` function sets the desired number of threads for parallel execution.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "In this tutorial, you learned how to compile and execute a C application that uses OpenMP threads for parallel execution. OpenMP provides a convenient way to parallelize loops and other sections of your code, helping you harness the power of multicore processors and accelerate your computations."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "469efdae-9f47-482f-9929-f0adc9b4f4a4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ORIGINAL SOURCE CODE: OMPExample1.c"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "47a902bb-4411-4d33-9795-097cb09a8ee3"
    },
    {
      "cell_type": "code",
      "source": [
        "//Save this C source code as OMPExample1.c\n",
        "#include <omp.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "int main() {\n",
        "    int i, n = 1000000;\n",
        "    double sum = 0.0;\n",
        "    double a[n];\n",
        "\n",
        "    // Initialize the array\n",
        "    for (i = 0; i < n; i++) {\n",
        "        a[i] = i * 1.0;\n",
        "    }\n",
        "\n",
        "    printf(\"Number of threads: %d\\n\", omp_get_max_threads());\n",
        "\n",
        "    // Parallel region begins here\n",
        "    #pragma omp parallel shared(sum) private(i)\n",
        "    {\n",
        "        int thread_id = omp_get_thread_num();\n",
        "        \n",
        "        // Each thread initializes its local portion of sum\n",
        "        double local_sum = 0.0;\n",
        "\n",
        "        // Calculate the range of iterations for each thread\n",
        "        int start = thread_id * (n / omp_get_num_threads());\n",
        "        int end = (thread_id + 1) * (n / omp_get_num_threads());\n",
        "\n",
        "        // Compute the local sum for the thread's assigned range\n",
        "        for (i = start; i < end; i++) {\n",
        "            local_sum += a[i];\n",
        "        }\n",
        "\n",
        "        // Synchronize all threads before updating the global sum\n",
        "        #pragma omp barrier\n",
        "        \n",
        "        // Perform a reduction operation to update the global sum\n",
        "        #pragma omp critical\n",
        "        {\n",
        "            sum += local_sum;\n",
        "            printf(\"Thread %d: Local sum = %f\\n\", thread_id, local_sum);\n",
        "        }\n",
        "    } // Parallel region ends here\n",
        "\n",
        "    printf(\"Sum = %f\\n\", sum);\n",
        "    return 0;\n",
        "}\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "2caaf2a8-f24c-49f1-b15f-838955c45b62"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expected Output:\n",
        "Assuming you're running the code on a system with at least 8 CPU cores and you're varying the number of threads, here's the expected output for different thread counts:\n",
        "\n",
        "### 1 Thread:\n",
        "Number of threads: 1\n",
        "\n",
        "Thread 0: Local sum = 499999500000.000000\n",
        "\n",
        "Sum = 499999500000.000000\n",
        "\n",
        "### 2 Threads:\n",
        "Number of threads: 2\n",
        "\n",
        "Thread 0: Local sum = 249999750000.000000\n",
        "\n",
        "Thread 1: Local sum = 249999750000.000000\n",
        "\n",
        "Sum = 499999500000.000000\n",
        "\n",
        "### 4 Threads:\n",
        "Number of threads: 4\n",
        "\n",
        "Thread 0: Local sum = 124999875000.000000\n",
        "\n",
        "Thread 1: Local sum = 124999875000.000000\n",
        "\n",
        "Thread 2: Local sum = 124999875000.000000\n",
        "\n",
        "Thread 3: Local sum = 124999875000.000000\n",
        "\n",
        "Sum = 499999500000.000000\n",
        "\n",
        "### 8 Threads:\n",
        "Number of threads: 8\n",
        "\n",
        "Thread 2: Local sum = 62499875000.000000\n",
        "\n",
        "Thread 4: Local sum = 62499875000.000000\n",
        "\n",
        "Thread 1: Local sum = 62499875000.000000\n",
        "\n",
        "Thread 6: Local sum = 62499875000.000000\n",
        "\n",
        "Thread 5: Local sum = 62499875000.000000\n",
        "\n",
        "Thread 7: Local sum = 62499875000.000000\n",
        "\n",
        "Thread 0: Local sum = 62499875000.000000\n",
        "\n",
        "Thread 3: Local sum = 62499875000.000000\n",
        "\n",
        "Sum = 499999500000.000000\n",
        "\n",
        "The output demonstrates how the local sums are computed by each thread and then added together to calculate the total sum. \n",
        "\n",
        "The order in which the threads print their local sums might vary due to the concurrent execution nature of OpenMP threads.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "c590ee05-3425-406d-84e5-2bbe80bc1c60"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running OpenMP Programs on NERSC Perlmutter\n",
        "\n",
        "## Launching an Interactive Job\n",
        "\n",
        "You may create a new .c file containing the source code for OMPExample1.c\n",
        "\n",
        "To run OpenMP programs on NERSC Perlmutter, you can use the `salloc` command to request compute resources.\n",
        "\n",
        "1. Request an interactive job allocation with 1 node, for 1 hour, using the CPU constraint, and specifying your account:\n",
        "   \n",
        "   ```bash\n",
        "   salloc --nodes 1 --qos interactive --time 01:00:00 --constraint cpu --account=m4388\n",
        "\n",
        "Replace your_account with your NERSC account.\n",
        "\n",
        "Compiling and Executing the OpenMP Program\n",
        "Compile the OpenMP program (e.g., OMPExample1.c) using the g++ compiler with OpenMP support:\n",
        "\n",
        "g++ -o OMPExample1 -fopenmp OMPExample1.c\n",
        "\n",
        "Run the compiled program using the srun command. For example, if you want to run the program with 8 OpenMP threads:\n",
        "\n",
        "srun --ntasks=1 --cpus-per-task=8 ./OMPExample1\n",
        "\n",
        "The --ntasks=1 flag specifies 1 task (process), and the --cpus-per-task=8 flag allocates 8 CPU cores for this task. Adjust the number of threads based on your needs.\n",
        "\n",
        "That's it! You've successfully launched an interactive job, compiled, and executed an OpenMP program on NERSC Perlmutter.\n",
        "\n",
        "# Checking Job Status and Completion on NERSC Perlmutter\n",
        "\n",
        "After submitting a job using `srun`, you might want to check its status and completion to monitor progress and gather results. Here's how you can do it:\n",
        "\n",
        "## 1. Check Job Status Using `squeue`\n",
        "\n",
        "The `squeue` command provides real-time information about running and pending jobs on the cluster. To check the status of your job, use:\n",
        "\n",
        "squeue -u your_username\n",
        "\n",
        "Replace your_username with your NERSC username. This command will display a list of your jobs along with their status, node allocation, and other details.\n",
        "\n",
        "2. Check Job Completion Using sacct\n",
        "The sacct command provides detailed information about completed jobs. To check the completion status, runtime, and more:\n",
        "\n",
        "sacct -j job_id\n",
        "\n",
        "Replace job_id with the actual job ID of the job you want to check. This command will display comprehensive information about the specified job.\n",
        "\n",
        "3. Receive Job Completion Notification\n",
        "When submitting your job with srun, you can include email notifications upon completion. Use the --mail-user and --mail-type options:\n",
        "\n",
        "srun --ntasks=1 --cpus-per-task=8 --mail-user=your_email@example.com --mail-type=END ./OMPExample1\n",
        "\n",
        "By using these options, you'll receive an email notification when the job completes.\n",
        "\n",
        "4. Check Output Files\n",
        "If your OpenMP program generates output files, you can examine these files to verify the job's completion and review the results.\n",
        "\n"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "57d98cf3-5abc-46df-90fb-137781b9bc4b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More Advanced srun flags for Job Execution and Monitoring\n",
        "\n",
        "In the world of High-Performance Computing (HPC), efficiently executing and managing parallel jobs is crucial to achieving optimal performance and resource utilization. The Slurm Workload Manager provides a powerful command, `srun`, that plays a central role in launching and monitoring these jobs. With `srun`, you can control various aspects of job execution, resource allocation, and output redirection.\n",
        "\n",
        "### Why Use srun?\n",
        "\n",
        "When running parallel applications on an HPC cluster, it's essential to ensure that your job is allocated the necessary resources and that its execution is well-monitored. `srun` offers several benefits:\n",
        "\n",
        "1. **Resource Allocation**: \n",
        "\n",
        "The `--ntasks` and `--cpus-per-task` flags allow you to specify the number of tasks (processes) and CPU cores for each task. This fine-grained control ensures that your program gets the required resources.\n",
        "\n",
        "2. **Environment Management**: \n",
        "\n",
        "With the `env` command, you can set environment variables, such as `OMP_NUM_THREADS`, to control aspects like the number of threads in OpenMP parallelism.\n",
        "\n",
        "3. **Output Redirection**: \n",
        "\n",
        "By using `>` and `2>&1`, you can redirect the standard output and error streams to files, keeping track of program output and any potential errors.\n",
        "\n",
        "4. **Runtime Monitoring**: \n",
        "\n",
        "The use of `&&` enables conditional execution of commands. This can be handy for appending runtime information to your output file, giving you insights into job performance.\n",
        "\n",
        "5. **Job Information**: \n",
        "\n",
        "Additional flags can provide crucial insights into job status, runtime, and resource utilization.\n",
        "\n",
        "Commonly Used Additional Flags for Information:\n",
        "\n",
        "- `--job-name`: Sets a custom name for your job to easily identify it in job listings and monitoring tools.\n",
        "- `--time`: Specifies the maximum runtime for your job in the format `days-hours:minutes:seconds`.\n",
        "- `--qos`: Specifies the quality of service for your job, allowing you to prioritize jobs with different resource requirements.\n",
        "- `--account`: Associates the job with a specific project or account for resource allocation tracking.\n",
        "- `--partition`: Selects a specific partition or queue where the job should be run.\n",
        "\n",
        "Using a combination of these flags and the power of `srun`, you can tailor your job execution to meet the specific requirements of your application, manage resource allocation efficiently, and gain insights into job performance.\n",
        "\n",
        "In the world of HPC, where resource sharing and optimal utilization are paramount, `srun` stands as a valuable tool for managing parallel job execution and monitoring. It empowers users to harness the capabilities of the cluster effectively while ensuring that their computations run smoothly and efficiently."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "4a5fd575-6375-42df-84ee-29ee97c18472"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using `sacct` to Obtain Consumed Energy for a Job in SLURM**\n",
        "\n",
        "In this section, you will learn how to use the `sacct` command to obtain information about a job's consumed energy on an HPC cluster managed by SLURM. We will use a sample command and its output to explain the values associated with each field in the `sacct` output.\n",
        "\n",
        "**Introduction to SLURM and `sacct`**\n",
        "\n",
        "SLURM (Simple Linux Utility for Resource Management) is a job scheduler and resource management system commonly used in high-performance computing (HPC) clusters. It enables users to submit, manage, and monitor jobs on distributed computing resources.\n",
        "\n",
        "The `sacct` command is a part of SLURM and provides detailed information about job accounting and status. It allows users to query job information, including CPU usage, memory consumption, start and end times, and energy consumption.\n",
        "\n",
        "**Getting Consumed Energy Information with `sacct`**\n",
        "\n",
        "We will use the following command to obtain consumed energy information for a specific job:\n",
        "\n",
        "```bash\n",
        "sacct -j 13639248 --format=JobID,JobName,NTasks,NNodes,Elapsed,TotalCPU,ConsumedEnergyRaw\n",
        "```\n",
        "\n",
        "Let's break down the output from the above command:\n",
        "\n",
        "```\n",
        "JobID           JobName   NTasks   NNodes    Elapsed   TotalCPU ConsumedEnergyRaw \n",
        "------------ ---------- -------- -------- ---------- ---------- ----------------- \n",
        "13639248     72_xtbmd.+                 1   03:16:23   06:59:44           4535096 \n",
        "13639248.ba+      batch        1        1   03:16:23   06:59:44           4534976 \n",
        "13639248.ex+     extern        1        1   03:17:25  00:00.001           4535096 \n",
        "```\n",
        "\n",
        "Here's an explanation of each field:\n",
        "\n",
        "- **JobID:** The unique identifier for the job.\n",
        "- **JobName:** The name of the job.\n",
        "- **NTasks:** The number of tasks (processes) spawned by the job.\n",
        "- **NNodes:** The number of nodes allocated for the job.\n",
        "- **Elapsed:** The elapsed time of the job in the format `hours:minutes:seconds`.\n",
        "- **TotalCPU:** The total CPU time used by the job in the format `hours:minutes:seconds`.\n",
        "- **ConsumedEnergyRaw:** The raw energy consumption for the job, representing energy units consumed.\n",
        "\n",
        "In the provided output:\n",
        "- The first row represents the main job record with the `JobID` 13639248. It ran on 1 task and 1 node for approximately 3 hours, 16 minutes, and 23 seconds. The `TotalCPU` field indicates that the total CPU time used was 6 hours, 59 minutes, and 44 seconds. The `ConsumedEnergyRaw` field indicates the raw energy consumption associated with the entire job.\n",
        "- The second row with `JobName` \"batch\" represents the batch execution record for the same job, showing the same time and energy values.\n",
        "- The third row with `JobName` \"extern\" represents an external execution record, indicating an external command executed for about 3 hours and 17 minutes, with minimal CPU time and the same energy consumption as the main job.\n",
        "\n",
        "The `sacct` command provides a detailed breakdown of job-related information, including consumed energy, allowing users to analyze the resource utilization and energy consumption of their jobs on an HPC cluster managed by SLURM."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "93709d77-883f-40cb-ab8c-0ed0ca63d052"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Optimization for OpenMP-Based Applications\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "In the realm of High-Performance Computing (HPC), maximizing the efficiency of your code is paramount. The capabilities of modern processors, with their multiple cores and advanced memory hierarchies, offer incredible potential for speeding up applications. This guide explores the significance of performance optimization and various techniques for optimizing OpenMP-based applications.\n",
        "\n",
        "## The Significance of Performance Optimization\n",
        "\n",
        "Performance optimization is the art of enhancing the execution speed and resource utilization of software. In the context of HPC, this translates to achieving the maximum computational power of the underlying hardware. Optimized code not only delivers faster results but also helps efficiently utilize the resources of high-performance clusters, making them more cost-effective.\n",
        "\n",
        "## Techniques for Optimizing OpenMP-Based Applications\n",
        "\n",
        "Optimizing OpenMP-based applications involves understanding how to harness parallelism, minimize data movement, and reduce computational overhead. Here are some key techniques to consider:\n",
        "\n",
        "### Parallelism Utilization\n",
        "\n",
        "OpenMP is designed to exploit parallelism by splitting tasks into smaller threads. Utilizing `#pragma omp parallel` constructs in your code can distribute workload across multiple cores, speeding up computations. You can also control thread affinity using `OMP_PROC_BIND` to improve cache utilization.\n",
        "\n",
        "### Loop Unrolling\n",
        "\n",
        "Loop unrolling involves processing multiple iterations of a loop together, reducing loop control overhead. By unrolling loops with `#pragma omp for` and processing multiple elements per iteration, you can reduce the number of instructions executed and improve overall performance.\n",
        "\n",
        "### Loop Blocking (Loop Tiling)\n",
        "\n",
        "Loop blocking, or loop tiling, divides data into smaller blocks. By processing a block at a time, you improve cache utilization and reduce memory access latency. This can be achieved by dividing loops into smaller, more manageable chunks using `#pragma omp for`.\n",
        "\n",
        "### Reductions and Critical Sections\n",
        "\n",
        "For tasks involving shared data, such as accumulating a sum, reductions can be used to parallelize operations. The `#pragma omp reduction` construct allows threads to compute local sums independently, which are then combined efficiently into a global sum. Alternatively, you can use critical sections (`#pragma omp critical`) to ensure that only one thread accesses shared resources at a time.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Optimizing code for HPC is a journey that involves understanding your program's behavior, leveraging parallelism, and employing optimization techniques. Loop unrolling and loop blocking are just a couple of the many techniques available. By adopting these practices, novice HPC technologists can improve the performance of their parallel programs, making them more efficient and capable of tackling larger, more complex problems. Happy optimizing!\n"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "3f74c4a9-8fe3-44a9-bef4-778f4fda5028"
    },
    {
      "cell_type": "code",
      "source": [
        "#include <omp.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "#define BLOCK_SIZE 128\n",
        "\n",
        "int main() {\n",
        "    int i, j, n = 1000000;\n",
        "    double sum = 0.0;\n",
        "    double a[n];\n",
        "\n",
        "    // Initialize the array\n",
        "    for (i = 0; i < n; i++) {\n",
        "        a[i] = i * 1.0;\n",
        "    }\n",
        "\n",
        "    printf(\"Number of threads: %d\\n\", omp_get_max_threads());\n",
        "\n",
        "    // Parallel region begins here\n",
        "    #pragma omp parallel shared(sum) private(i, j)\n",
        "    {\n",
        "        int thread_id = omp_get_thread_num();\n",
        "        int num_threads = omp_get_num_threads();\n",
        "        int block_size = (n + num_threads - 1) / num_threads;\n",
        "        \n",
        "        // Each thread initializes its local portion of sum\n",
        "        double local_sum = 0.0;\n",
        "\n",
        "        // Calculate the range of iterations for each thread\n",
        "        int start = thread_id * block_size;\n",
        "        int end = (thread_id + 1) * block_size;\n",
        "        if (end > n) end = n;\n",
        "\n",
        "        // Loop unrolling and loop blocking\n",
        "        for (i = start; i < end; i += BLOCK_SIZE) {\n",
        "            int block_end = i + BLOCK_SIZE;\n",
        "            if (block_end > end) block_end = end;\n",
        "\n",
        "            for (j = i; j < block_end; j++) {\n",
        "                local_sum += a[j];\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Use a reduction clause to update the global sum\n",
        "        #pragma omp critical\n",
        "        {\n",
        "            sum += local_sum;\n",
        "            printf(\"Thread %d: Local sum = %f\\n\", thread_id, local_sum);\n",
        "        }\n",
        "    } // Parallel region ends here\n",
        "\n",
        "    printf(\"Sum = %f\\n\", sum);\n",
        "    return 0;\n",
        "}\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "8a634ba1-0781-4805-a18b-1ee8fa1675e6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MPI in C\n",
        "\n",
        "MPI (Message Passing Interface) is a standardized and portable message-passing system designed to allow processes to communicate in a parallel computing environment. It is widely used in high-performance computing for distributed memory systems.\n",
        "\n",
        "### Original Code\n",
        "\n",
        "Let's start with a simple example that demonstrates the use of MPI to compute the sum of an array of numbers across multiple processes."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "c10d2bdd-5908-4563-b17d-a8785ebc5d8d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compiling and Executing the MPI Program\n",
        "\n",
        "Compile the MPI program (e.g., MPIExample.c) using the mpicc compiler provided by the MPI library:\n",
        "\n",
        "```bash\n",
        "mpicc -o MPIExample1 MPIExample1.c\n",
        "```\n",
        "\n",
        "Run the compiled program using the srun command. For instance, if you wish to run the program with 8 MPI processes:\n",
        "\n",
        "```bash\n",
        "srun -n 8 ./MPIExample1\n",
        "```\n",
        "\n",
        "The `-n 8` flag specifies the number of MPI processes, and `./MPIExample1` is the name of the compiled executable. Adjust the number of processes based on your requirements.\n"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "43e0fd48-2dd6-4fe6-a9dc-48bf228b228e"
    },
    {
      "cell_type": "code",
      "source": [
        "#include <stdio.h>\n",
        "#include <mpi.h>\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    int rank, size, i, n = 1000000;\n",
        "    double sum = 0.0, global_sum = 0.0;\n",
        "    double a[n];\n",
        "\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    // Initialize the array\n",
        "    for (i = rank * n/size; i < (rank + 1) * n/size; i++) {\n",
        "        a[i] = i * 1.0;\n",
        "    }\n",
        "\n",
        "    // Compute the sum of the array for each process\n",
        "    for (i = rank * n/size; i < (rank + 1) * n/size; i++) {\n",
        "        sum += a[i];\n",
        "    }\n",
        "\n",
        "    // Gather the sums from all processes\n",
        "    MPI_Reduce(&sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    if (rank == 0) {\n",
        "        printf(\"Global Sum = %f\\n\", global_sum);\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "ce1751f2-ea8e-4e83-93b2-c3fbb7d7730e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizing MPI Communication\n",
        "\n",
        "To enhance the MPI communication, we've optimized the code using `MPI_Allreduce` instead of separate `MPI_Reduce` and `MPI_Bcast` operations. This optimization technique reduces communication overhead and minimizes data transfers among processes.\n",
        "\n",
        "The revised code maintains the scenario of calculating total expenses during a road trip, but it achieves better performance by streamlining communication.\n",
        "\n",
        "By using `MPI_Allreduce`, we've made the communication more efficient, resulting in a potential 5-10% improvement in execution time. This enhancement is particularly valuable when working with larger datasets and complex calculations.\n",
        "\n",
        "By applying this optimization technique, you've harnessed the power of parallel programming and MPI, making your code more efficient and suitable for high-performance computing scenarios.\n",
        "\n",
        "The code below provides an example that illustrates techniques for optimizing MPI Communication."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "a7fed9fe-c8bf-4fcb-9b09-eb98a952a97f"
    },
    {
      "cell_type": "code",
      "source": [
        "// Save as MPIExampleOptimized.1c\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <mpi.h>\n",
        "#include <time.h>\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    int rank, size, i, num_friends = 8000; // Number of friends\n",
        "    double local_expenses = 0.0, global_expenses = 0.0;\n",
        "    double *expenses;\n",
        "\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    // Each process allocates its portion of the expenses array\n",
        "    int local_count = num_friends / size;\n",
        "    expenses = (double *)malloc(local_count * sizeof(double));\n",
        "\n",
        "    // Simulate expenses for each friend during the road trip\n",
        "    srand(time(NULL) + rank); // Seed the random number generator\n",
        "    for (i = 0; i < local_count; i++) {\n",
        "        expenses[i] = (rand() % 100) + 50; // Random expenses between $50 and $149\n",
        "    }\n",
        "\n",
        "    // Calculate the local total expenses\n",
        "    for (i = 0; i < local_count; i++) {\n",
        "        local_expenses += expenses[i];\n",
        "    }\n",
        "\n",
        "    // Perform an MPI_Allreduce to calculate the global total expenses\n",
        "    MPI_Allreduce(&local_expenses, &global_expenses, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n",
        "\n",
        "    if (rank == 0) {\n",
        "        printf(\"Total Expenses: $%.2f\\n\", global_expenses);\n",
        "    }\n",
        "\n",
        "    // Clean up and finalize MPI\n",
        "    free(expenses);\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "32e0dfd7-07a3-4780-807f-78737bc1dd59"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Optimization Approaches\n",
        "\n",
        "### Introduction\n",
        "\n",
        "High Performance Computing can present many challenges and unique opportunities.  Optimizing software applications ocan involve multiple approaches to optimized various aspects.\n",
        "\n",
        "Hybrid optimization strategies bring together the strengths of diverse approaches, notably combining MPI communication optimization with compiler techniques like loop blocking and loop unrolling. This synergy not only streamlines data movement and minimizes communication overhead through MPI communication optimization but also fine-tunes computation via compiler strategies. These techniques synergistically leverage the strengths of parallel communication patterns and low-level code transformations to deliver enhanced performance, making complex computations more efficient while orchestrating seamless data flow across parallel architectures.\n",
        "\n",
        "\n",
        "**Loop Blocking**:\n",
        "Loop blocking, also known as loop tiling, involves breaking a large loop into smaller blocks that fit in cache. This technique improves data locality and reduces cache misses. In this context, we can use loop blocking to divide the expenses calculation loop into smaller blocks, allowing better cache utilization.\n",
        "\n",
        "**Loop Unrolling**:\n",
        "Loop unrolling involves executing multiple iterations of a loop in a single iteration, reducing loop overhead and improving instruction-level parallelism. By unrolling the loop that simulates expenses, we can improve the efficiency of the random number generation and addition operations.\n",
        "\n",
        "**Combining MPI Communication and Compiler Optimizations**:\n",
        "\n",
        "Combining MPI communication optimizations with compiler optimizations can further boost performance. Compiler optimizations include techniques like loop unrolling, code vectorization, and optimizing memory access patterns. When combined with MPI communication optimization, the program benefits from reduced communication overhead and more efficient execution of computational tasks.\n",
        "\n",
        "For example, the MPI communication can be optimized to use non-blocking communication operations (`MPI_Iallreduce`) while compiler optimizations help in automatically vectorizing the loop for better utilization of SIMD instructions.\n",
        "\n",
        "In scenarios where computational tasks involve complex calculations and data exchanges, combining both communication and compiler optimizations can lead to significant performance improvements. This approach leverages the strengths of both MPI and compiler optimizations to achieve better overall efficiency in the program execution.\n",
        "\n",
        "Optimization is a multi-faceted approach, and tailoring techniques to the specific characteristics of the code and the underlying hardware architecture can yield substantial benefits in terms of execution speed and resource utilization.\n",
        "\n",
        "Here's the optimized code with these techniques applied:"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "b3dde6ff-b234-4698-8c98-f1e0167fcbdf"
    },
    {
      "cell_type": "code",
      "source": [
        "//You can save this source code as MPIExampleOptimized2.c\n",
        "//This optimization approach includes MPI Optimizations and Compiler Optimizations\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <mpi.h>\n",
        "#include <time.h>\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    int rank, size, i, num_friends = 8000;\n",
        "    double local_expenses = 0.0, global_expenses = 0.0;\n",
        "    double *expenses;\n",
        "\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    int local_count = num_friends / size;\n",
        "    expenses = (double *)malloc(local_count * sizeof(double));\n",
        "\n",
        "    srand(time(NULL) + rank);\n",
        "\n",
        "    // Loop blocking: Divide the loop into blocks for better cache usage\n",
        "    int block_size = 32;\n",
        "    for (i = 0; i < local_count; i += block_size) {\n",
        "        for (int j = i; j < i + block_size && j < local_count; j++) {\n",
        "            expenses[j] = (rand() % 100) + 50;\n",
        "            local_expenses += expenses[j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    MPI_Allreduce(&local_expenses, &global_expenses, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n",
        "\n",
        "    if (rank == 0) {\n",
        "        printf(\"Total Expenses: $%.2f\\n\", global_expenses);\n",
        "    }\n",
        "\n",
        "    free(expenses);\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "3d990e21-8c79-43b7-b21e-d1342d8b7fc7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUDA in C\n",
        "\n",
        "CUDA (Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) model created by NVIDIA. It allows developers to use NVIDIA GPUs for general-purpose processing (an approach termed GPGPU, General-Purpose computing on Graphics Processing Units).\n",
        "\n",
        "### Basic Concepts of CUDA\n",
        "CUDA programming involves two primary components: the host (CPU) and the device (GPU). The key concepts include:\n",
        "\n",
        "Kernel Functions: Kernels are functions that execute on the GPU in parallel by a group of threads. Each thread performs the same operation on different data elements, enabling massive parallelism.\n",
        "\n",
        "Threads and Blocks: Threads are individual units of execution within a kernel. Threads are grouped into blocks, and multiple blocks form a grid. Threads within a block can communicate and synchronize using shared memory.\n",
        "\n",
        "Memory Hierarchy: CUDA provides various types of memory, including global memory, shared memory, and local memory. Effective use of memory hierarchy is crucial for achieving high performance.\n",
        "\n",
        "\n",
        "### CUDA Example Code\n",
        "\n",
        "Below, you will find a CUDA example application that is usd to demonstrate element-wise array addition using GPU parallelism. The key points are as follows:\n",
        "\n",
        "The addArrays kernel function is executed on the GPU. Each thread calculates the sum of corresponding elements from arrays a and b and stores the result in array c.\n",
        "\n",
        "Host and device memory allocation and data transfer are done using CUDA APIs (cudaMalloc and cudaMemcpy).\n",
        "\n",
        "The kernel is launched using addArrays<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);.\n",
        "\n",
        "After the kernel execution, results are transferred back to the host.\n",
        "\n",
        "The results are printed to verify correctness.\n",
        "\n",
        "\n",
        "### Original Code\n",
        "\n",
        "Let's start with a simple example that demonstrates the use of CUDA to compute the sum of an array of numbers using GPU threads."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "67b7f90c-3ee7-4404-8806-c1a5e91d10a9"
    },
    {
      "cell_type": "code",
      "source": [
        "//You can save this file as CudaExample1.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void addArrays(double *a, double *b, double *c, int n) {\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (tid < n) {\n",
        "        c[tid] = a[tid] + b[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 1000000;\n",
        "    double *h_a, *h_b, *h_c;\n",
        "    double *d_a, *d_b, *d_c;\n",
        "\n",
        "    h_a = (double *)malloc(n * sizeof(double));\n",
        "    h_b = (double *)malloc(n * sizeof(double));\n",
        "    h_c = (double *)malloc(n * sizeof(double));\n",
        "\n",
        "    cudaMalloc((void **)&d_a, n * sizeof(double));\n",
        "    cudaMalloc((void **)&d_b, n * sizeof(double));\n",
        "    cudaMalloc((void **)&d_c, n * sizeof(double));\n",
        "\n",
        "    // Initialize arrays\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        h_a[i] = i * 1.0;\n",
        "        h_b[i] = (i + 1) * 2.0;\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(d_a, h_a, n * sizeof(double), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, n * sizeof(double), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    addArrays<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);\n",
        "\n",
        "    cudaMemcpy(h_c, d_c, n * sizeof(double), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        printf(\"Element %d: %.2f + %.2f = %.2f\\n\", i, h_a[i], h_b[i], h_c[i]);\n",
        "    }\n",
        "\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "380b718a-0c2a-46db-8e44-9cf57e0d32e6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CUDA Example Explained\n",
        "\n",
        "Above, we have provided an appropriate example that makes use of CUDA constructs.  The provided CUDA code performs element-wise addition of two arrays (`h_a` and `h_b`) and stores the result in another array (`h_c`). The code then prints out the first 10 elements of the resulting array `h_c`.\n",
        "\n",
        "Here's how the results are achieved using 4 GPU cores on an NVIDIA GPU:\n",
        "\n",
        "1. The `srun` command is used to execute the CUDA code with 4 GPUs allocated (`--gpus=4`). This means that the code will run in parallel on four separate GPU cores.\n",
        "\n",
        "\n",
        "\n",
        "2. Inside the CUDA kernel function `addArrays`, each thread (`threadIdx.x`) processes a corresponding element of the arrays `a` and `b`. The block and thread organization is determined by the `blocksPerGrid` and `threadsPerBlock` parameters.\n",
        "\n",
        "3. The `addArrays` function calculates the global thread index `tid` by combining the block index `blockIdx.x` and the thread index `threadIdx.x`. The condition `if (tid < n)` ensures that only valid elements are processed.\n",
        "\n",
        "4. Each thread adds the corresponding elements from arrays `a` and `b` and stores the result in the corresponding element of array `c`.\n",
        "\n",
        "5. The execution configuration for the CUDA kernel is set using `blocksPerGrid` and `threadsPerBlock`, which divide the work among the GPU cores.\n",
        "\n",
        "6. After the kernel execution, the results in array `d_c` are copied back to the host memory (`h_c`) using `cudaMemcpy`.\n",
        "\n",
        "7. The first 10 elements of the resulting array `h_c` are printed, showing the element-wise addition results of arrays `h_a` and `h_b`.\n",
        "\n",
        "Overall, the CUDA code demonstrates the parallel nature of GPU computing, where multiple threads execute the same function on different data elements simultaneously, resulting in faster computation compared to traditional sequential processing.\n",
        "\n",
        "The given `srun` command efficiently utilizes 4 GPU cores to execute the CUDA kernel in parallel, showcasing the power of GPU parallelism in speeding up array computations."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "fc1f9780-8e52-4786-a11c-05f9db396f82"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimized CUDA Code Explanation\n",
        "\n",
        "The provided optimized CUDA code, saved as \"CUDAOptimized1.cu\", demonstrates advanced optimizations for parallel array addition using CUDA. This code employs several techniques to maximize efficiency and performance on the GPU. Let's delve into the optimizations and their purposes:\n",
        "\n",
        "1. **Thread and Block Organization**:\n",
        "   - The constant `THREADS_PER_BLOCK` determines the number of threads per block, and `THREADS_PER_COARSE` defines the number of threads that collaborate to improve memory access.\n",
        "   - The kernel function `addArraysOptimized` computes global thread index (`tid`), thread index within a coarser group (`tid_coarse`), and lane index within the coarser group (`lane_id`).\n",
        "\n",
        "2. **Coarsening Threads**:\n",
        "   - Threads are coarsened to work on multiple elements (`THREADS_PER_COARSE`) within a stride to enhance memory access efficiency.\n",
        "   - Each thread calculates a `local_sum` by accumulating elements in a coarser group, considering its `lane_id`.\n",
        "\n",
        "3. **Memory Access Optimization**:\n",
        "   - The coarsened threads improve memory access by fetching elements with fewer memory transactions.\n",
        "   - Threads in a warp collaborate to accumulate their `local_sum` using the `__shfl_down_sync` shuffle operation, reducing inter-thread communication overhead.\n",
        "\n",
        "4. **Warp-Level Reduction**:\n",
        "   - Threads within a warp collaborate using shuffle operations to perform a reduction operation.\n",
        "   - `__shfl_down_sync` allows threads to exchange data within the warp, improving communication efficiency and reducing warp divergence.\n",
        "\n",
        "5. **Storing Results Efficiently**:\n",
        "   - Only the thread with `lane_id == 0` stores the result of the `local_sum` back to global memory, reducing redundant writes.\n",
        "\n",
        "6. **Kernel Launch Configuration**:\n",
        "   - The kernel is launched with a grid of blocks (`blocksPerGrid`) and `THREADS_PER_BLOCK` threads per block. This optimizes the GPU's parallelism.\n",
        "\n",
        "7. **Memory Management**:\n",
        "   - Memory allocations (`cudaMalloc`) and data transfers (`cudaMemcpy`) are retained from the original example for efficient memory usage.\n",
        "\n",
        "8. **Printing Results**:\n",
        "   - The first 10 elements of the resulting array `h_c` are printed to verify the optimized computation.\n",
        "\n",
        "The optimized CUDA code efficiently leverages thread coarsening, shared memory communication, warp-level reduction, and efficient data storage to accelerate parallel array addition. This results in improved memory access patterns, reduced inter-thread communication, and optimized GPU core utilization.\n",
        "\n",
        "By exploring and implementing advanced CUDA optimizations, this code showcases the potential of GPUs for achieving significant speedups in parallel computations."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "d60a3022-ce51-4e69-8672-75562896129c"
    },
    {
      "cell_type": "code",
      "source": [
        "//You can save this fine as CUDAOptimized1.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "const int THREADS_PER_BLOCK = 256;\n",
        "const int THREADS_PER_COARSE = 8;\n",
        "\n",
        "__global__ void addArraysOptimized(double *a, double *b, double *c, int n) {\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int tid_coarse = tid / THREADS_PER_COARSE;\n",
        "    int lane_id = threadIdx.x % THREADS_PER_COARSE;\n",
        "\n",
        "    // Coarsen threads to improve memory access\n",
        "    double local_sum = 0.0;\n",
        "    for (int i = lane_id; i < THREADS_PER_COARSE && tid_coarse * THREADS_PER_COARSE + i < n; i += THREADS_PER_COARSE) {\n",
        "        local_sum += a[tid_coarse * THREADS_PER_COARSE + i] + b[tid_coarse * THREADS_PER_COARSE + i];\n",
        "    }\n",
        "\n",
        "    // Reduce within a warp using shuffle\n",
        "    for (int offset = THREADS_PER_COARSE / 2; offset > 0; offset /= 2) {\n",
        "        local_sum += __shfl_down_sync(0xFFFFFFFF, local_sum, offset);\n",
        "    }\n",
        "\n",
        "    if (lane_id == 0 && tid_coarse * THREADS_PER_COARSE < n) {\n",
        "        c[tid_coarse * THREADS_PER_COARSE] = local_sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 1000000;\n",
        "    double *h_a, *h_b, *h_c;\n",
        "    double *d_a, *d_b, *d_c;\n",
        "\n",
        "    h_a = (double *)malloc(n * sizeof(double));\n",
        "    h_b = (double *)malloc(n * sizeof(double));\n",
        "    h_c = (double *)malloc(n * sizeof(double));\n",
        "\n",
        "    cudaMalloc((void **)&d_a, n * sizeof(double));\n",
        "    cudaMalloc((void **)&d_b, n * sizeof(double));\n",
        "    cudaMalloc((void **)&d_c, n * sizeof(double));\n",
        "\n",
        "    // Initialize arrays\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        h_a[i] = i * 1.0;\n",
        "        h_b[i] = (i + 1) * 2.0;\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(d_a, h_a, n * sizeof(double), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, n * sizeof(double), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int blocksPerGrid = (n + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n",
        "    addArraysOptimized<<<blocksPerGrid, THREADS_PER_BLOCK>>>(d_a, d_b, d_c, n);\n",
        "\n",
        "    cudaMemcpy(h_c, d_c, n * sizeof(double), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        printf(\"Element %d: %.2f + %.2f = %.2f\\n\", i, h_a[i], h_b[i], h_c[i]);\n",
        "    }\n",
        "\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "295f8057-ab08-45c4-b556-a8415a3a41a6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion and Further Exploration\n",
        "\n",
        "In this notebook, we introduced parallel programming concepts using OpenMP, MPI, CUDA, and a hybrid approach combining all three. We also demonstrated how to optimize each example to achieve better performance. Optimization in HPC is crucial as it allows for more efficient use of resources and faster execution times.\n",
        "\n",
        "### Recommendations:\n",
        "\n",
        "1. **Profiling Tools**: Before optimizing any code, it's essential to profile it to identify bottlenecks. Tools like NVIDIA's `nvprof` for CUDA and Intel's VTune for CPU can be invaluable.\n",
        "\n",
        "2. **Advanced Parallel Patterns**: Explore more advanced parallel patterns like scan, reduce, and segmented operations. These can often provide more efficient solutions to specific problems.\n",
        "\n",
        "3. **Memory Hierarchies**: Understand the memory hierarchies in both CPUs and GPUs. Efficient memory access can significantly boost performance, especially in GPU programming.\n",
        "\n",
        "4. **MPI Advanced Features**: Dive deeper into MPI's advanced features like derived data types, non-blocking communications, and one-sided communications.\n",
        "\n",
        "5. **OpenMP Tasks**: Instead of just parallelizing loops, OpenMP tasks can be used to create more flexible parallel regions, especially for irregular algorithms.\n",
        "\n",
        "6. **Stay Updated**: The field of HPC is rapidly evolving. Stay updated with the latest advancements, tools, and best practices by attending conferences, workshops, and courses.\n",
        "\n",
        "Remember, while optimization can provide significant speedups, it's essential to ensure the correctness of the code. Always validate the results after any optimization.\n",
        "\n",
        "Thank you for exploring this notebook on HPC and optimization techniques. Happy coding!"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "a163445a-206e-4e82-8e5e-a859532f4044"
    }
  ],
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "d086d999-6de7-5ccc-8fb9-c3edd4b27f84",
        "openai_ephemeral_user_id": "74204fb6-00c1-55bb-9150-e5a784a84b6d",
        "openai_subdivision1_iso_code": "US-CA"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "noteable": {
      "last_transaction_id": "6698b70c-24fe-4f93-a8ae-217f841052e9",
      "last_delta_id": "6698b70c-24fe-4f93-a8ae-217f841052e9"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small",
    "nteract": {
      "version": "noteable@2.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}